{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality-Reduction-Techniques.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb6sqksRYOSO"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JN0gh9Hzgip",
        "outputId": "72dd8485-d167-4920-9db3-23e6769ba1bb"
      },
      "source": [
        "pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkJr7SY6rq8j"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxabmCq6whOy"
      },
      "source": [
        "dataset = pd.read_csv('/content/Wine.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "I6O1YX5exhPD",
        "outputId": "27e4bfbb-1bc6-465a-8d29-50a7a5910a7f"
      },
      "source": [
        "pd.set_option('display.max_columns',50)\n",
        "dataset.head(8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic_Acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Ash_Alcanity</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total_Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid_Phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color_Intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280</th>\n",
              "      <th>Proline</th>\n",
              "      <th>Customer_Segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>14.20</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.45</td>\n",
              "      <td>15.2</td>\n",
              "      <td>112</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.97</td>\n",
              "      <td>6.75</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1450</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>14.39</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.45</td>\n",
              "      <td>14.6</td>\n",
              "      <td>96</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2.52</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.98</td>\n",
              "      <td>5.25</td>\n",
              "      <td>1.02</td>\n",
              "      <td>3.58</td>\n",
              "      <td>1290</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>14.06</td>\n",
              "      <td>2.15</td>\n",
              "      <td>2.61</td>\n",
              "      <td>17.6</td>\n",
              "      <td>121</td>\n",
              "      <td>2.60</td>\n",
              "      <td>2.51</td>\n",
              "      <td>0.31</td>\n",
              "      <td>1.25</td>\n",
              "      <td>5.05</td>\n",
              "      <td>1.06</td>\n",
              "      <td>3.58</td>\n",
              "      <td>1295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
              "0    14.23        1.71  2.43          15.6        127           2.80   \n",
              "1    13.20        1.78  2.14          11.2        100           2.65   \n",
              "2    13.16        2.36  2.67          18.6        101           2.80   \n",
              "3    14.37        1.95  2.50          16.8        113           3.85   \n",
              "4    13.24        2.59  2.87          21.0        118           2.80   \n",
              "5    14.20        1.76  2.45          15.2        112           3.27   \n",
              "6    14.39        1.87  2.45          14.6         96           2.50   \n",
              "7    14.06        2.15  2.61          17.6        121           2.60   \n",
              "\n",
              "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
              "0        3.06                  0.28             2.29             5.64  1.04   \n",
              "1        2.76                  0.26             1.28             4.38  1.05   \n",
              "2        3.24                  0.30             2.81             5.68  1.03   \n",
              "3        3.49                  0.24             2.18             7.80  0.86   \n",
              "4        2.69                  0.39             1.82             4.32  1.04   \n",
              "5        3.39                  0.34             1.97             6.75  1.05   \n",
              "6        2.52                  0.30             1.98             5.25  1.02   \n",
              "7        2.51                  0.31             1.25             5.05  1.06   \n",
              "\n",
              "   OD280  Proline  Customer_Segment  \n",
              "0   3.92     1065                 1  \n",
              "1   3.40     1050                 1  \n",
              "2   3.17     1185                 1  \n",
              "3   3.45     1480                 1  \n",
              "4   2.93      735                 1  \n",
              "5   2.85     1450                 1  \n",
              "6   3.58     1290                 1  \n",
              "7   3.58     1295                 1  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr1edJyFyDyM",
        "outputId": "aa749a22-964a-423c-bbc1-8f961eea2539"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "N6kdtxNDyNBU",
        "outputId": "52e5a804-ede4-4c1d-ab4d-68c263118c37"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic_Acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Ash_Alcanity</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total_Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid_Phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color_Intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280</th>\n",
              "      <th>Proline</th>\n",
              "      <th>Customer_Segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13.000618</td>\n",
              "      <td>2.336348</td>\n",
              "      <td>2.366517</td>\n",
              "      <td>19.494944</td>\n",
              "      <td>99.741573</td>\n",
              "      <td>2.295112</td>\n",
              "      <td>2.029270</td>\n",
              "      <td>0.361854</td>\n",
              "      <td>1.590899</td>\n",
              "      <td>5.058090</td>\n",
              "      <td>0.957449</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>746.893258</td>\n",
              "      <td>1.938202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.811827</td>\n",
              "      <td>1.117146</td>\n",
              "      <td>0.274344</td>\n",
              "      <td>3.339564</td>\n",
              "      <td>14.282484</td>\n",
              "      <td>0.625851</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>0.572359</td>\n",
              "      <td>2.318286</td>\n",
              "      <td>0.228572</td>\n",
              "      <td>0.709990</td>\n",
              "      <td>314.907474</td>\n",
              "      <td>0.775035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>278.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.362500</td>\n",
              "      <td>1.602500</td>\n",
              "      <td>2.210000</td>\n",
              "      <td>17.200000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>1.742500</td>\n",
              "      <td>1.205000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>3.220000</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>1.937500</td>\n",
              "      <td>500.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.050000</td>\n",
              "      <td>1.865000</td>\n",
              "      <td>2.360000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>2.355000</td>\n",
              "      <td>2.135000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>4.690000</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>2.780000</td>\n",
              "      <td>673.500000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>13.677500</td>\n",
              "      <td>3.082500</td>\n",
              "      <td>2.557500</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>2.875000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>1.120000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>985.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.830000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.230000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>3.880000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>3.580000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1680.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Alcohol  Malic_Acid         Ash  Ash_Alcanity   Magnesium  \\\n",
              "count  178.000000  178.000000  178.000000    178.000000  178.000000   \n",
              "mean    13.000618    2.336348    2.366517     19.494944   99.741573   \n",
              "std      0.811827    1.117146    0.274344      3.339564   14.282484   \n",
              "min     11.030000    0.740000    1.360000     10.600000   70.000000   \n",
              "25%     12.362500    1.602500    2.210000     17.200000   88.000000   \n",
              "50%     13.050000    1.865000    2.360000     19.500000   98.000000   \n",
              "75%     13.677500    3.082500    2.557500     21.500000  107.000000   \n",
              "max     14.830000    5.800000    3.230000     30.000000  162.000000   \n",
              "\n",
              "       Total_Phenols  Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  \\\n",
              "count     178.000000  178.000000            178.000000       178.000000   \n",
              "mean        2.295112    2.029270              0.361854         1.590899   \n",
              "std         0.625851    0.998859              0.124453         0.572359   \n",
              "min         0.980000    0.340000              0.130000         0.410000   \n",
              "25%         1.742500    1.205000              0.270000         1.250000   \n",
              "50%         2.355000    2.135000              0.340000         1.555000   \n",
              "75%         2.800000    2.875000              0.437500         1.950000   \n",
              "max         3.880000    5.080000              0.660000         3.580000   \n",
              "\n",
              "       Color_Intensity         Hue       OD280      Proline  Customer_Segment  \n",
              "count       178.000000  178.000000  178.000000   178.000000        178.000000  \n",
              "mean          5.058090    0.957449    2.611685   746.893258          1.938202  \n",
              "std           2.318286    0.228572    0.709990   314.907474          0.775035  \n",
              "min           1.280000    0.480000    1.270000   278.000000          1.000000  \n",
              "25%           3.220000    0.782500    1.937500   500.500000          1.000000  \n",
              "50%           4.690000    0.965000    2.780000   673.500000          2.000000  \n",
              "75%           6.200000    1.120000    3.170000   985.000000          3.000000  \n",
              "max          13.000000    1.710000    4.000000  1680.000000          3.000000  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqh-9PCuyD4z",
        "outputId": "62d9a87e-7f1a-40e2-d04b-3e0276d5a9db"
      },
      "source": [
        "dataset.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Alcohol                 float64\n",
              "Malic_Acid              float64\n",
              "Ash                     float64\n",
              "Ash_Alcanity            float64\n",
              "Magnesium                 int64\n",
              "Total_Phenols           float64\n",
              "Flavanoids              float64\n",
              "Nonflavanoid_Phenols    float64\n",
              "Proanthocyanins         float64\n",
              "Color_Intensity         float64\n",
              "Hue                     float64\n",
              "OD280                   float64\n",
              "Proline                   int64\n",
              "Customer_Segment          int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qmju2kIyKKY",
        "outputId": "2e34009c-c3c7-4e51-817e-5989abe08e76"
      },
      "source": [
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Alcohol                 0\n",
              "Malic_Acid              0\n",
              "Ash                     0\n",
              "Ash_Alcanity            0\n",
              "Magnesium               0\n",
              "Total_Phenols           0\n",
              "Flavanoids              0\n",
              "Nonflavanoid_Phenols    0\n",
              "Proanthocyanins         0\n",
              "Color_Intensity         0\n",
              "Hue                     0\n",
              "OD280                   0\n",
              "Proline                 0\n",
              "Customer_Segment        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PEZ7YH5KOEb"
      },
      "source": [
        "X = dataset.drop(['Customer_Segment'], axis=1)\n",
        "y = dataset['Customer_Segment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-UCD7ezAJG2"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLJ8xAf4bjBO"
      },
      "source": [
        "# dimensionality reduction techniques (Feature Extraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9qyOJ00dZJo"
      },
      "source": [
        "## Principal component analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGonBTQedb83"
      },
      "source": [
        "The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjVG-nLi1P1r"
      },
      "source": [
        "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space. It tries to preserve the essential parts that have more variation of the data and remove the non-essential parts with fewer variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txx35ti01P1s"
      },
      "source": [
        "Dimensions are nothing but features that represent the data. For example, A 28 X 28 image has 784 picture elements (pixels) that are the dimensions or features which together represent that image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn9ZCmm-1P1t"
      },
      "source": [
        "One important thing to note about PCA is that it is an Unsupervised dimensionality reduction technique, you can cluster the similar data points based on the feature correlation between them without any supervision (or labels), and you will learn how to achieve this practically using Python in later sections of this tutorial!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CefBjBfA1P1t"
      },
      "source": [
        "According to Wikipedia, PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAtRihqn1P1u"
      },
      "source": [
        "### When to use PCA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D35kXAx1P1u"
      },
      "source": [
        "Data Visualization: When working on any data related problem, the challenge in today's world is the sheer volume of data, and the variables/features that define that data. To solve a problem where data is the key, you need extensive data exploration like finding out how the variables are correlated or understanding the distribution of a few variables. Considering that there are a large number of variables or dimensions along which the data is distributed, visualization can be a challenge and almost impossible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs7LwmHY1P1v"
      },
      "source": [
        "Speeding Machine Learning (ML) Algorithm: Since PCA's main idea is dimensionality reduction, you can leverage that to speed up your machine learning algorithm's training and testing time considering your data has a lot of features, and the ML algorithm's learning is too slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uu15xQ1P1v"
      },
      "source": [
        "### How to do PCA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP8jCQ9u1P1v"
      },
      "source": [
        "We can calculate a Principal Component Analysis on a dataset using the PCA() class in the scikit-learn library. The benefit of this approach is that once the projection is calculated, it can be applied to new data again and again quite easily.\n",
        "\n",
        "When creating the class, the number of components can be specified as a parameter.\n",
        "\n",
        "The class is first fit on a dataset by calling the fit() function, and then the original dataset or other data can be projected into a subspace with the chosen number of dimensions by calling the transform() function.\n",
        "\n",
        "Once fit, the eigenvalues and principal components can be accessed on the PCA class via the explained_variance_ and components_ attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBVb9a1Ydny_",
        "outputId": "5384b12b-0a3c-4e06-b5d0-6cab14f0a4b7"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K3076nFfFQo",
        "outputId": "a79b21a7-5788-4ce6-da8e-d2d128c97b05"
      },
      "source": [
        "pca.explained_variance_ratio_.reshape(len(pca.explained_variance_ratio_),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36198848],\n",
              "       [0.1920749 ],\n",
              "       [0.11123631],\n",
              "       [0.0706903 ],\n",
              "       [0.06563294],\n",
              "       [0.04935823],\n",
              "       [0.04238679],\n",
              "       [0.02680749],\n",
              "       [0.02222153],\n",
              "       [0.01930019],\n",
              "       [0.01736836],\n",
              "       [0.01298233],\n",
              "       [0.00795215]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlgJ2rABf2S4"
      },
      "source": [
        "this is the explained variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkrbDkpi4-Ic"
      },
      "source": [
        "Remove Constant, Quasi Constant and Duplicate Features (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3cZlbk5G7M"
      },
      "source": [
        "##remove constant and quasi constant features\n",
        "#constant_filter = VarianceThreshold(threshold=0.01)\n",
        "#constant_filter.fit(X_train)\n",
        "#X_train_filter = constant_filter.transform(X_train)\n",
        "#X_test_filter = constant_filter.transform(X_test)\n",
        "\n",
        "##remove duplicate features\n",
        "#X_train_T = X_train_filter.T\n",
        "#X_test_T = X_test_filter.T\n",
        "\n",
        "#X_train_T = pd.DataFrame(X_train_T)\n",
        "#X_test_T = pd.DataFrame(X_test_T)\n",
        "\n",
        "#duplicated_features = X_train_T.duplicated()\n",
        "\n",
        "#features_to_keep = [not index for index in duplicated_features]\n",
        "\n",
        "#X_train_unique = X_train_T[features_to_keep].T\n",
        "#X_test_unique = X_test_T[features_to_keep].T\n",
        "\n",
        "##from here you can scale data\n",
        "##...\n",
        "\n",
        "#X_train_unique = pd.DataFrame(X_train_unique)\n",
        "#X_test_unique = pd.DataFrame(X_test_unique)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lTbh68l4m09"
      },
      "source": [
        "use the code below to see the accuracy of the each number of the given n_component using pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFfbvdR91P14"
      },
      "source": [
        "#for component in range(1,30):\n",
        "#    pca = PCA(n_components=component, random_state=42)\n",
        "#    pca.fit(X_train_uncorr)\n",
        "#    X_train_pca = pca.transform(X_train_uncorr)\n",
        "#    X_test_pca = pca.transform(X_test_uncorr)\n",
        "#    print('Selected Components: ', component)\n",
        "#    run an estimator and use its score function to see how many component is better \n",
        "#    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P51gGH623MkA"
      },
      "source": [
        "use the function below to Remove the correlated Feature after the Remove of Constant, Quasi Constant and Duplicate Features (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOlMA7VE1P10"
      },
      "source": [
        "#corrmat = X_train_unique.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJEzZFsy1P10"
      },
      "source": [
        "#find correlated features\n",
        "#def get_correlation(data, threshold):\n",
        "#    corr_col = set()\n",
        "#    corrmat = data.corr()\n",
        "#    for i in range(len(corrmat.columns)):\n",
        "#        for j in range(i):\n",
        "#            if abs(corrmat.iloc[i, j]) > threshold:\n",
        "#                colname = corrmat.columns[i]\n",
        "#                corr_col.add(colname)\n",
        "#    return corr_col\n",
        "\n",
        "#corr_features = get_correlation(X_train_unique, 0.70)\n",
        "#print('correlated features: ', len(set(corr_features)) )    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w16EAt561P11"
      },
      "source": [
        "#X_train_uncorr = X_train_unique.drop(labels=corr_features, axis = 1)\n",
        "#X_test_uncorr = X_test_unique.drop(labels = corr_features, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7TptfJc4O15"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSvPLWEsgZD0",
        "outputId": "b08a9adb-026a-4efb-8cf5-e4cafbe99f8e"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAaHbd_Zg03B"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niwQ3wfqgdR6"
      },
      "source": [
        "X_pca = X.copy()\n",
        "X_pca = pca.transform(X_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtfJWXV7hXo6"
      },
      "source": [
        "## Non-negative matrix factorization (NMF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnfgxDiShqEg"
      },
      "source": [
        "NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, such as astronomy. NMF is well known since the multiplicative update rule by Lee & Seung, which has been continuously developed: the inclusion of uncertainties, the consideration of missing data and parallel computation, sequential construction which leads to the stability and linearity of NMF, as well as other updates including handling missing data in digital image processing.\n",
        "\n",
        "\n",
        "With a stable component basis during construction, and a linear modeling process, sequential NMF is able to preserve the flux in direct imaging of circumstellar structures in astromony, as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar discs. In comparison with PCA, NMF does not remove the mean of the matrices, which leads to unphysical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6RqaEnj9wB"
      },
      "source": [
        "The init attribute determines the initialization method applied, which has a great impact on the performance of the method. NMF implements the method Nonnegative Double Singular Value Decomposition. NNDSVD 4 is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aubR1eSrhbJ0",
        "outputId": "518f67a1-b81a-490e-ec62-bf98c57c8e73"
      },
      "source": [
        "X_not_scaled = dataset.drop(['Customer_Segment'], axis=1)\n",
        "from sklearn.decomposition import NMF\n",
        "nmf = NMF(max_iter= 3000,init='nndsvda')\n",
        "nmf.fit(X_not_scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(init='nndsvda', max_iter=3000)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0kz8sNpkYTd",
        "outputId": "ba507739-7529-45df-8de0-7bbd1fd5782d"
      },
      "source": [
        "nmf.components_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 8.75506675e-02, 2.01445590e-01, 0.00000000e+00,\n",
              "        9.21217595e-02, 0.00000000e+00, 1.59707944e-02, 7.23076034e-02,\n",
              "        1.06240719e+02],\n",
              "       [1.85606647e+01, 5.88742524e+00, 3.95323934e+00, 4.20329485e+01,\n",
              "        2.67475227e+02, 2.39667876e+00, 3.00891126e-01, 6.41956870e-01,\n",
              "        2.31944739e+00, 7.18238666e+00, 1.32668134e+00, 2.34410540e+00,\n",
              "        3.35043749e+01],\n",
              "       [5.26213758e+00, 0.00000000e+00, 1.04002591e+00, 5.26181446e+00,\n",
              "        9.70336585e+01, 1.04385985e+00, 1.64077189e+00, 8.44710937e-02,\n",
              "        1.23790947e+00, 0.00000000e+00, 8.63363301e-01, 1.92079458e+00,\n",
              "        0.00000000e+00],\n",
              "       [7.05612636e+01, 7.44295365e+01, 3.62633507e+01, 6.95851800e+02,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.07913887e+01,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.26834528e+00, 4.71624971e+00,\n",
              "        8.72132307e+01],\n",
              "       [1.42241328e+01, 2.96491935e+00, 4.84907689e+00, 7.44792398e+01,\n",
              "        0.00000000e+00, 4.88553453e+00, 5.37264462e+00, 1.36920905e+00,\n",
              "        2.57077514e+00, 1.04683444e+01, 0.00000000e+00, 4.96232167e+00,\n",
              "        6.63139900e+01],\n",
              "       [2.75245547e+02, 6.74009038e+01, 5.25566454e+01, 5.10247255e+02,\n",
              "        9.89122754e+02, 3.50623094e+01, 2.05839669e-02, 1.54305792e+01,\n",
              "        2.82868645e+01, 1.78822966e+02, 1.57112228e+01, 2.90306543e+01,\n",
              "        1.14631176e-01],\n",
              "       [2.20861862e+00, 6.54469778e-01, 2.53646837e-02, 0.00000000e+00,\n",
              "        1.14372894e+01, 0.00000000e+00, 0.00000000e+00, 1.94945269e-01,\n",
              "        4.31999543e-01, 5.92462077e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.84155516e+02],\n",
              "       [1.37581141e+01, 2.18976127e+01, 1.66009771e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.32639178e-01,\n",
              "        0.00000000e+00, 2.78049183e+01, 0.00000000e+00, 0.00000000e+00,\n",
              "        9.14319959e+02],\n",
              "       [3.61998262e+02, 6.80972697e+01, 3.04207902e+01, 1.95395173e+02,\n",
              "        0.00000000e+00, 5.01652094e+01, 4.43215115e+01, 8.65053458e+00,\n",
              "        2.66960669e+01, 0.00000000e+00, 2.69444075e+01, 8.99460589e+01,\n",
              "        3.64066138e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 8.43751632e+02, 6.29621184e+02, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        4.02693616e+04],\n",
              "       [2.18367312e+00, 3.22417204e-01, 4.96617207e-01, 3.39726323e+00,\n",
              "        2.55105559e+01, 4.64937703e-01, 5.56327644e-01, 1.00544640e-01,\n",
              "        3.12023163e-01, 1.52652725e+00, 2.29028704e-01, 3.40980618e-01,\n",
              "        3.63869738e+02],\n",
              "       [4.60338096e+00, 5.42668791e-01, 7.82074433e-01, 4.88506514e+00,\n",
              "        3.57020839e+01, 9.44047704e-01, 8.72469856e-01, 9.66755001e-02,\n",
              "        5.82250109e-01, 1.99089981e+00, 2.99807254e-01, 1.00572716e+00,\n",
              "        3.72826932e+02],\n",
              "       [2.17842624e+02, 2.90034734e+01, 3.81346298e+01, 2.61298293e+02,\n",
              "        1.65370520e+03, 3.83163774e+01, 3.35802938e+01, 3.71704108e+00,\n",
              "        2.52603259e+01, 9.54628425e+01, 1.61462182e+01, 4.32761286e+01,\n",
              "        1.33761977e+04]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs9llyPJlc6y"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHEoztASlgSM",
        "outputId": "5befda1d-1640-4c12-b4dc-9824b10333da"
      },
      "source": [
        "nmf = NMF(max_iter= 3000,init='nndsvda',n_components=2)\n",
        "nmf.fit(X_not_scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(init='nndsvda', max_iter=3000, n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeUDW8evmYT8"
      },
      "source": [
        "also there are other parameters that you can change.\n",
        "\n",
        "https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27e082VopQj"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLq8yGZDlkIN"
      },
      "source": [
        "X_nmf = X_not_scaled.copy()\n",
        "X_nmf = nmf.transform(X_not_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1bL3pWkmjh6"
      },
      "source": [
        "## Kernel PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNntc-rYmuvq"
      },
      "source": [
        "Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbpXYqbkm-5q"
      },
      "source": [
        "in this example we use Gaussian RBF kernel but you can tune the parameters and other kernels as you like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZWIfyTEmt93",
        "outputId": "ba0df2ee-8a2e-4b2b-e89e-bf59113631b4"
      },
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "kpca = KernelPCA(kernel='rbf')\n",
        "kpca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KernelPCA(kernel='rbf')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARsY9xpRoWUc"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIsGqas6oGrM",
        "outputId": "a6b0927b-8331-4129-ca11-244c8c82ae66"
      },
      "source": [
        "kpca = KernelPCA(kernel='rbf',n_components=2)\n",
        "kpca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KernelPCA(kernel='rbf', n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwc-duyCoqNd"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXoZgUSfojBF"
      },
      "source": [
        "X_kpca = X.copy()\n",
        "X_kpca = kpca.transform(X_kpca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxTZ__6FpORE"
      },
      "source": [
        "## Factor Analysis (FA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G9Xyys-poJb"
      },
      "source": [
        "A simple linear generative model with Gaussian latent variables.\n",
        "\n",
        "\n",
        "The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.\n",
        "\n",
        "\n",
        "If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain PPCA.\n",
        "\n",
        "\n",
        "FactorAnalysis performs a maximum likelihood estimate of the so-called loading matrix, the transformation of the latent variables to the observed ones, using SVD based approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SIhUEU6qQ7f"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHWYQZptsa5-"
      },
      "source": [
        "there are other parameters that we can change.\n",
        "\n",
        "https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvtJcVyHpp2-",
        "outputId": "34141e9c-37ec-4e80-b8e9-b92b3798d1be"
      },
      "source": [
        "from sklearn.decomposition import FactorAnalysis as FA\n",
        "fa = FA(n_components=2)\n",
        "fa.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FactorAnalysis(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs6gFGu9rYmX"
      },
      "source": [
        "The estimated noise variance for each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0cY-NW_q0w9",
        "outputId": "b05018a7-3c39-4197-c8b9-58a3abe79f86"
      },
      "source": [
        "fa.noise_variance_.reshape(len(fa.noise_variance_),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.45207798],\n",
              "       [0.76073034],\n",
              "       [0.89401611],\n",
              "       [0.84035367],\n",
              "       [0.85260876],\n",
              "       [0.19827767],\n",
              "       [0.07950909],\n",
              "       [0.68543965],\n",
              "       [0.55634331],\n",
              "       [0.1880347 ],\n",
              "       [0.4970161 ],\n",
              "       [0.24305993],\n",
              "       [0.45818261]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yfns2yPsF3j"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOt_32-Zq4vd"
      },
      "source": [
        "X_fa = X.copy()\n",
        "X_fa = fa.transform(X_fa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2-4Dxqes9CM"
      },
      "source": [
        "## Independent Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u10NPqM2tP-1"
      },
      "source": [
        "Independent Component Analysis (ICA) is a machine learning technique to separate independent sources from a mixed signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXT1-hRWtcQS"
      },
      "source": [
        "Unlike principal component analysis which focuses on maximizing the variance of the data points, the independent component analysis focuses on independence, i.e. independent components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK22fovitswi"
      },
      "source": [
        "you can choose number of independent components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn-O9qjpuZKm"
      },
      "source": [
        "there are other parameters that we can change. or give your own functions.\n",
        "\n",
        "https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02QqtpKVtQfU",
        "outputId": "be6c3d40-f6e9-407e-d26c-7074da745665"
      },
      "source": [
        "from sklearn.decomposition import FastICA as ICA\n",
        "ica = ICA(n_components=2)\n",
        "ica.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FastICA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rJIqQrDvSh4"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oul_7otYuuFM"
      },
      "source": [
        "X_ica = X.copy()\n",
        "X_ica = ica.transform(X_ica)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cmUh1hXvqJn"
      },
      "source": [
        "## \tIncremental principal components analysis (IPCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vyfriII1zIQ"
      },
      "source": [
        "Incremental principal components analysis (IPCA).\n",
        "\n",
        "Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n",
        "\n",
        "Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA, and allows sparse input.\n",
        "\n",
        "This algorithm has constant memory complexity, on the order of batch_size * n_features, enabling use of np.memmap files without loading the entire file into memory. For sparse matrices, the input is converted to dense in batches (in order to be able to subtract the mean) which avoids storing the entire dense matrix at any one time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwyX-aYP2Mk7"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fmJ42JA5LDT"
      },
      "source": [
        "like PCA \n",
        "\n",
        "explained_variance_ratio_ attribute shows Percentage of variance explained by each of the selected components. If all components are stored, the sum of explained variances is equal to 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySjE7AH31z5X",
        "outputId": "7f7adfa8-1237-4bfb-f955-d53d07e8f152"
      },
      "source": [
        "from sklearn.decomposition import IncrementalPCA as IPCA\n",
        "ipca = IPCA(n_components=2)\n",
        "ipca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncrementalPCA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ge1vKhx6nMT"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQmY_2PH5n-A"
      },
      "source": [
        "X_ipca = X.copy()\n",
        "X_ipca = ipca.transform(X_ipca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqhnNFRGAmy_"
      },
      "source": [
        "## Mini-batch Sparse Principal Components Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE-WGIzsAp6d"
      },
      "source": [
        "Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P5ktDq2EZnO"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmrBWtF3Ao7a",
        "outputId": "e7e90060-4ca2-4738-9e01-bf4f47a65719"
      },
      "source": [
        "from sklearn.decomposition import MiniBatchSparsePCA as MBSPCA\n",
        "mbspca = MBSPCA(n_components=2)\n",
        "mbspca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniBatchSparsePCA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VegWZ0LTEi_o"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKJ2vFFLEeku"
      },
      "source": [
        "X_mbspca = X.copy()\n",
        "X_mbspca = mbspca.transform(X_mbspca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tG01uCwE_IV"
      },
      "source": [
        "## Sparse Principal Components Analysis (SparsePCA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFdy-YdVFH5a"
      },
      "source": [
        "Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbPGHR9LFKG-"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3O8eDxUFLXG"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQtQraxhFIa9",
        "outputId": "fed89900-951a-49cb-c78b-da2a7f1d6d81"
      },
      "source": [
        "from sklearn.decomposition import SparsePCA as SPCA\n",
        "spca = SPCA(n_components=2)\n",
        "spca.fit(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparsePCA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwFg_LBuHVuY"
      },
      "source": [
        "X_spca = X.copy()\n",
        "X_spca = spca.transform(X_spca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cO2k1OWHg2_"
      },
      "source": [
        "## truncated SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBJl6qnAIMW9"
      },
      "source": [
        "Dimensionality reduction using truncated SVD (aka LSA).\n",
        "\n",
        "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWjfCb0Ie0j"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Y_dqBhIjxy"
      },
      "source": [
        "like PCA \n",
        "\n",
        "explained_variance_ratio_ attribute shows Percentage of variance explained by each of the selected components. If all components are stored, the sum of explained variances is equal to 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA6tuygIIf0V"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrzR2ReiILcj"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD as LSA\n",
        "lsa = LSA(n_components=2)\n",
        "lsa.fit(X)\n",
        "X_lsa = X.copy()\n",
        "X_lsa = lsa.transform(X_lsa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgmGQpXeJG_Z"
      },
      "source": [
        "## Linear Discriminant Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAMSDbao1P1q"
      },
      "source": [
        "The idea behind LDA is simple. Mathematically speaking, we need to find a new feature space to project the data in order to maximize classes separability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZpHmxjn1P1q"
      },
      "source": [
        "Linear Discriminant Analysis is a supervised algorithm as it takes the class label into consideration. It is a way to reduce dimensionality while at the same time preserving as much of the class discrimination information as possible.\n",
        "\n",
        "LDA helps you find the boundaries around clusters of classes. It projects your data points on a line so that your clusters are as separated as possible, with each cluster having a relative (close) distance to a centroid.\n",
        "\n",
        "So the question arises- how are these clusters are defined and how do we get the reduced feature set in case of LDA?\n",
        "\n",
        "Basically LDA finds a centroid of each class datapoints. For example with thirteen different features LDA will find the centroid of each of its class using the thirteen different feature dataset. Now on the basis of this, it determines a new dimension which is nothing but an axis which should satisfy two criteria:\n",
        "1.\tMaximize the distance between the centroid of each class.\n",
        "2.\tMinimize the variation (which LDA calls scatter and is represented by s2), within each category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfLtK0UrJWUW"
      },
      "source": [
        "Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n",
        "\n",
        "A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes rule.\n",
        "\n",
        "The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.\n",
        "\n",
        "The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip-o9ofRKOGC"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7XA1mXRKVaI"
      },
      "source": [
        "explained_variance_ratio_ attribute\n",
        "\n",
        "Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or svd solver is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lrRlpUDKXxG"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmH1bVFpJI3g",
        "outputId": "5944ca4c-bb88-4b88-a28e-6d879197fe22"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA(n_components=2)\n",
        "lda.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxk89sWHKFz1"
      },
      "source": [
        "X_lda = X.copy()\n",
        "X_lda = lda.transform(X_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgAYt6CaKncL"
      },
      "source": [
        "## t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1C13WtUSY8K"
      },
      "source": [
        "T-distributed Stochastic Neighbor Embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_X-cDxVMDFv"
      },
      "source": [
        "T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j2p9Jt7SVvd"
      },
      "source": [
        "t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
        "\n",
        "It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maatens FAQ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noEcz_KNTTao"
      },
      "source": [
        "you can choose number of components(features) you want to have by changing the n_component parameter to number that you  want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzEgAn98TUnO"
      },
      "source": [
        "then use transform method to create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSu70WUoMBZt"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = X.copy()\n",
        "X_tsne = tsne.fit_transform(X_tsne)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGqdWwqTWIDP"
      },
      "source": [
        "## UMAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy3VPkNxWLeG"
      },
      "source": [
        "Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73U1_MSXWjtP",
        "outputId": "c7c724ab-f61e-4b9e-f688-afd95e510a8e"
      },
      "source": [
        "pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.24.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abA_LfRIWK29"
      },
      "source": [
        "from umap import UMAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqlwxA1kW_Wn"
      },
      "source": [
        "there are parameters that may be important depending to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-hpCB4nWfg7",
        "outputId": "7fe6ed51-fac2-402d-c4b3-d30acb653584"
      },
      "source": [
        "reducer = UMAP(n_components=2)\n",
        "X_umap = X.copy()\n",
        "embedding = reducer.fit_transform(X_umap)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5OrbDNtYjXt"
      },
      "source": [
        "## Others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "242zTUTzYnI_"
      },
      "source": [
        "there are other techniques that you can read online like Random projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20n1m8tSjtXL"
      },
      "source": [
        "i like to say something about Random Projection\n",
        "\n",
        "Random Projections are a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes.\n",
        "\n",
        "The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.\n",
        "\n",
        "The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia):\n",
        "\n",
        "In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection."
      ]
    }
  ]
}